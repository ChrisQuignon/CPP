\section{Implementation}
\label{sec:implementation}
Sadly it was not possible to create a proper parallel implementation of the project in the given time, i will however present the results of the serial version. The problems and challenges are presented in the last subsection.

\subsection{Libraries}
\label{sec:libs}
This section highlights all libraries used in this project. Besides the standard library for exploiting the cell processor, two additional libraries were used. One to handle the bmp file which holds the generated data and one to handle large numbers.

\subsubsection{gmplib}
As stated in section \ref{sec:number_size}, the project requires the handling of arbitrary large numbers. Best practice here is "The GNU Multiple Precision Arithmetic Library" which handles arbitrary large and precise numbers. The library also provides several number theoretic functions that were used in the context of prime numbers. This point will be described in section \ref{sec:prim_test}.

\subsubsection{libbmp}
To create, manipulate and save bmp files, only few libraries exist. It is more common to find these functionality in a larger context merged with other image formats. This is because the inefficiency of the bmp format. In the field is is almost extinct in favor to the png, gif or jpeg format. Bmp does not use any compression and is straight forward in the way it stores the image information. It mainly consists of a file header, an information header and the image information block. This block may include a color table, but may as well be the pure color information per pixel.

This simple procedure comes in handy to the needs of this project. A fast and plain storage solution for data that is not expected to gain big advantages from compression. The choice fell on the "libbmp - BMP library" published under the GNU Lesser General Public License on \url{http://code.google.com/p/libbmp/}. The used Version 0.1.3 is unfinished and lacks of several features but allows easy and fundamental changes without big efforts. Without any restriction on function usage, it was possible to manipulate underlying data structures and extract parts of the data without concerning about nested or static functions allowing a fast implementation.

This advantage had to be payed with some overhead at the bmp file size. For the most parts, the manipulated image information was binary. The only interesting thing was, if a pixel represents a prime number on not. This suggest a monochrome bmp, where one bit represents one pixel. This however was not featured by the library, but was neither by other libraries of similar size. The one bit information is now stored in one byte.

\subsection{Space filling curves}
\label{sec:curves}
\par{Definition}
A space filling curve is defined as a path through every position in a discrete multi dimensional space with equal edge-length.%cube like

To these space filling curves, several other criteria can be applied:
\begin{itemize}%enumerate
   \item Dimensions\\
      To how many dimension does the curve apply
   \item Continuity\\
      The path does only jump to adjacency squares
   \item Recursive\\
      The path can be generated by recursive application of a simple pattern
   \item Size restriction\\
      The curve can be scaled to any size, without losing any of its other criteria
\end{itemize}%enumerate

The creation of the curves to which the primes were aligned to gave some challenges which will be examined in this section. At the beginning, the theoretical problem of the primality test to decide if a number is a prime or not is discussed. Afterward the two implemented algorithms to produce a space filing curve are explained. The second algorithm which implements the Hilbert curve holds as an example of how to implement other space filling curves like the e curve or the n curve. If the curve can not be built by stepping from one field on the bitmap to an adjacent field, more advanced algorithms have to be used, that require more complex computation of the steps. Jumping from one flied to an adjacent in respect to the current state of the pattern however is highly serial. A parallel version would use divide and conquer strategies.

\subsubsection{Primality Test}
\label{sec:prim_test}
As described in section \ref{sec:primality_test}, the primality test is highly inefficient if it is not probabilistic. The implementation of an own primality test that can handle the number representation of the \emph{gmplib} would have cost too much time. Luckily, the very same library already provided a primality test. The Miller-Rabin probabilistic primality test is applied several times to minimize the probability of false positives in the prime pattern. This however remains possible. However, the probability that several of these false positives are aligned as a pattern within the large test scope is neglectable and can be falsified by applying a non heuristic primality test.

\subsubsection{Spiral Algorithm}
The algorithm to align the prime numbers in a spiral around the center of the bitmap is fairly simple. At first, the center has to be determined and set. To actually have a center represented by a pixel, the bitmap height and width has to be odd. With every iteration of the algorithm one additional circumnavigation is done. By starting in one direction the first and second line - which lies rectangular to the first - have the length of one. The next two lines that continue the spiral in the same direction - either clockwise or counter clockwise - have one additional pixel. This pixel is the center that has not been passed by the first two lines. After these 4 line have been drawn, the length of the next line is also increased by one. In a two dimensional space the four rectangular direction have also been passed, thus the spiral is now facing in the very same direction as at the beginning. The algorithm now repeats until the calculated amount of iteration or wished pixed have been passed.

\subsubsection{Hilbert Curve}
\graphic{hilbert_curve}{width=10.5cm}{Three Iteration of the Hilbert curve.}
The Hilbert curve is 2 dimensional, continuous and recursive, but restricted to space sizes of $2^{n}$. The curves can be spaced to higher dimensions.

To understand the algorithm for the Hilbert curve it is crucial to envision the derivations and changes between the single steps of the Hilbert curve. One iteration of the Hilbert curve passes a two by two pixel grid with the origin at the lower left pixel in this order: (0, 0), (1, 0), (1, 1), (0, 1). This is done by the transition of the given Axis (X, Y), starting at (0, 0) in this order: (+1, --), (-- , +1), (-1, --). These pixel scale transition are represented by a triangle facing in the direction to which the transition is done (\tt, \tl, \tb, \tr). So for traversing 4 pixels three transition were made.

This transition is represented by the symbol \sqr. This transition order may as well be done facing in other directions, and is then represented by the symbols \sqt, \sql, \sqb. Every one of these open squares have their own transition order. Their notation is as follows:

\begin{itemize}%enumerate
   \item \sqr $\Rightarrow$ \tr \tt \tl
   \item \sqt $\Rightarrow$ \tt \tr \tb
   \item \sql $\Rightarrow$ \tl \tb \tr
   \item \sqb $\Rightarrow$ \tb \tl \tt
\end{itemize}%enumerate

If the Hilbert curve is done with several iterations, the transition of the open square are interleaved with open squares as follows:

\begin{itemize}%enumerate
   \item \sqr $\Rightarrow$ \sqt \tr \sqr \tt \sqr \tl \sqb
   \item \sqt $\Rightarrow$ \sqr \tt \sqt \tr \sqt \tb \sql
   \item \sql $\Rightarrow$ \sqb \tl \sql \tb \sql \tr \sqt
   \item \sqb $\Rightarrow$ \sql \tb \sqb \tl \sqb \tt \sqr
\end{itemize}%enumerate

Thus, the Hilbert curve is described as a grammar where the open squares are nonterminals and the triangles are terminals. However, the information of how often the nonterminal have to be exchanged has to be set globally and marks the number of iterations.

\graphic{hilbertalgorithm}{width=6cm}{Two Iteration of the Hilbert curve grammar in tree form.}

Thus a Hilbert curve of three iterations in generated as follows:
\boxit{
\sqt \hskip 10 pt  $\Rightarrow$ \\
\sqr \tt \sqt \tr \sqt \tb \sql \hskip 10 pt $\Rightarrow$ \\
\sqt \tr \sqr \tt \sqr \tl \sqb \hskip 10 pt \tt \hskip 10 pt \sqr \tt \sqt \tr \sqt \tb \sql \hskip 10 pt \tr \hskip 10 pt \sql \tb \sqb \tl \sqb \tt \sqr \hskip 10 pt \tb \hskip 10 pt \sqb \tl \sql \tb \sql \tr \sqt \hskip 10 pt $\Rightarrow$ \\
\tt \tr \tb \tr \tr \tt \tl \tt \tr \tt \tl \tl \tb \tl \tt \hskip 10 pt \tt \\
\tr \tt \tl \tt \tt \tr \tb \tr \tt \tr \tb \tb \tl \tb \tr \hskip 10 pt \tr \\
\tl \tb \tr \tb \tb \tl \tt \tl \tb \tl \tt \tt \tr \tt \tl \hskip 10 pt \tb \\
\tb \tl \tt \tl \tl \tb \tr \tb \tl \tb\tr \tr \tt \tr \tb 
}{hilbert_grammar}{Derivation of three iterations of the Hilbert curve.}

Similar grammars can be done for other space filling curves.

\subsection{Pattern Detection}
\label{sec:pattern}
The identification of lines within the Ulam spiral and other traversions of the prime number sequence is set as a goal of this report. It is necessary to give numbers to the fact that one can see lines in the pattern. For further studies of the prime number sequence the identification of these lines can assist improved primality test that rely on the mathematical functions of the lines found in the Ulam spiral.
At first it is necessary to get an idea of what these seen lines mean in a mathematical way. The vector through a picture can be described as a mathematical function or an algorithm that traverses this vector. In the Ulam spiral these vectors lay at multiples of 45° to the basis of the picture.
If one of this vectors can be identified by the eye it means that there are significantly more points on this vector than on the others. It also may be that the closes parallel vector to the seen one have significant less points and therefor the seen one has a high contrast.
It is difficult to identify these line with pattern detection algorithms from the field of image processing. There one can find a variety of edge detection algorithms like the Laplace filter or the Sobel operator. These algorithm rely on continuous lines and blur the original image. This is not as precise and reliable to enable further utilization.
The given implementation give a probability for every vector. So every vector has a statistic value that approximate the probability that a given point on this vector is prime number or not. The cumulation of several crossing vector then can produce an even more accurate probability that a point represents a prime or not.
For small images or at the edges of an image where a 45° vector just holds few values this probability of course is inaccurate and has to be discarded. However it was possible to clearly locate high probability vectors within the images. The corresponding figures can be found in section \ref{sec:results}.

\subsection{Parallel implementation}
\label{sec:par_imp}
The implementation of the parallel version was very challenging and it turned out be be too complex to be finished in the given time. I will explain these challenges and how i faced them. Some were implemented successfully, some had to stay in the design phase as reported in section \ref{sec:paralleldesign}. I ascribe my failure to a personal insufficiency of experience in programming the Cell processor in C and not to a misconceptions in the design or defective assumptions of the software. To approve this i will give a short word on my experiences in fore run to the project and then explain three major challenges i faced.
I learned C for this project and favor higher abstractions programming languages. Starting with Java and Lisp  and after some dissipations with Python and Prolog currently have most experience with Haskell which is functional, pure and strictly typed. From there - without stating any superiority to any of these languages - i am not very skilled with implicit type conversions and pointer arithmetics. Both of them are heavy used when programing for the Cell processor and both are core parts of my following narrated problem. I was able to identify the problems and came up with solutions but it took my too much time to implement all of them which finally lead to an incomplete implementation.

\subsubsection{Pointers}
The explicit and implicit use of pointers are essential when writing code for the Cell processor and when dealing with copious data structure like bitmaps which hold pixel arrays and header information at different non subsequently heap spaces. Pointers are also used to pass and address data in the main storage which can not be accessed by the SPE. It has to transfer the data to its own local store. The clear distinction between the data and pointers to this data or within this data is critical. The arithmetics on this data also has to be figured out carefully. Especially because own data structures have to be aligned when they need to be passed between the SPU and the PPU or vice versa. This alignment and multiple type definitions and their conversion without data changes of the very same bit vector on the machine has to be tracked carefully. The data may be interlaced with zeros to meet the alignment criteria and one can not rely on the data size within the data structure to deduce the pointer to the data within the structure without extensive testing.
The interpretation of the data behind a type definition may also vary from the expectations one may have which is hard to identify if the data is represented as zero on the output but actually is not as it was the case with some pointers when printed on the standard output.

\subsubsection{Two dimensional arrays}
Two dimensional array are often used in the program. To access a single pixel of the bitmap the two dimensional array is an obvious choice because the two dimensions resemble the two dimensions of the image. To conclude from this analogousness that an array is one block of data is faulty.
However the two dimensional array serves as a great data structure to exploit the Cell processor architecture. The distinction between the first dimension which is an array of pointers and the second dimension which is the actual data referenced by the first dimension comes in handy to the polling mechanism which is triggered by the SPE to get data from the main memory. The typically large data behind a two dimensional array can - and probably has to be - obtained in a couple of requests. The transfer of the first dimension of the array already holds the pointers needed to request portions of the data needed.
The trick here is to be aware of the different sizes the data has in the first and in the second dimension and that every data lies within the second dimension. Pointers can only navigate in one of the many arrays of the second dimension and conclusions from one second dimension array pointer to another are only possible if the difference between the references of the referencing pointers in the fist dimension are taken into account. I suggest not to conclude any pointer positions in two distinct second dimension array but to traverse every single array. 

\subsubsection{Data transfer size}
The two functions used to transfer data from and to the SPEs are \emph{mfc\_ get} and \emph{mfc\_put}. Both of them are restricted in the size. This restriction however is not the local memory limit and the data required to transfer may exceed this limitation. For both of these functions wrappers were implemented that can be requested to transfer arbitrary large data. They split the data in half can recursively call themselves on the bisect data until the data size is fallen below the limit.
% These aspects include data transfer and data alignment.
% 2d array
% pixel size

%\subsectioncd {subsection}
%\subsubsection{subsubsection}
%\paragraph{paragraph}
%\subparagraph{subparagraph}
%List
%\begin{itemize}%enumerate
%	\item Punkt1
%	\item Punkt2
%\end{itemize}%enumerate

%\graphic{label}{caption}

%Ref to Picture:
%(see:\ref{fig:XXX})

%Table
%\begin{table}[H]
%\begin{longtable} {| l l  | }
%\hline
%Topic1		& Topic2 	\\ \hline
%Cell1		& Cell2		\\
%Cell1.1	& Cell2.1	\\
%\hline
%\end{longtable}
%\end{table}
