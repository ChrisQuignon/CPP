\section{Implementation}
\label{sec:implementation}
Unfortunately it was not possible to create a proper parallel implementation of the project in the given time. I will, however, present the results of the serial version.
At first, the used libraries are introduced and their selection justified.
In the second part, the theoretical problem of the primality test to decide if a number is a prime or not is discussed. Afterwards the two implemented algorithms to produce a space filling curve are explained. The second algorithm which implements the Hilbert curve holds as an example of how to implement other space filling curves like the e curve or the n curve. Pattern detection is the penultimate point of this section. The decision not to use edge detection but hard math is outlined. The very last and largest point is about the difficulties and challenges of the implementation of the parallel version.

\subsection{Libraries}
\label{sec:libs}
This section highlights all libraries imported to create this project. Besides the standard library for exploiting the cell processor \emph{libspe} and \emph{mfcio} as introduced in \cite{cellguide}, two additional libraries were used. One to handle the bmp file which holds the generated data and one to handle large numbers.

\subsubsection{gmplib}
As stated in section \ref{sec:number_size}, the project requires the handling of arbitrary large numbers. Best practice here is "The GNU Multiple Precision Arithmetic Library" which handles arbitrary large and precise numbers. The library also provides several number theoretic functions - including the used primality test - that were used in the context of prime numbers. This point will be described in section \ref{sec:prim_test}.

\subsubsection{libbmp}
To create, manipulate and save bmp files, only few libraries exist. It is more common to find these functionality in a larger context merged with other image formats. This is because of the inefficiency of the bmp format. In the field it is almost extinct in favor to the \emph{png}, \emph{gif} or \emph{jpeg} format. Bmp does not use any compression and is straight forward with the method to store the image information. As set by the bmp standard as in \cite{bmp}, the bmp consists of a file header, an information header and the image information block. This block may include a color table and references to this table, but may as well be the pure color information for every pixel.

This simple procedure comes in handy to the needs of this project, a plain storage solution for data that is fast and easy to adapt and use. The images generated by the software are not expected to gain great advantages from compression. The choice fell on the "libbmp - BMP library" published under the GNU Lesser General Public License on \url{http://code.google.com/p/libbmp/}. The used Version 0.1.3 is unfinished and lacks several features but allows easy and fundamental changes without big effort. Without any restriction on function usage, it was possible to manipulate underlying data structures and extract parts of the data without concerning about nested or static functions allowing a fast integration and implementation changes.

This advantage had to be payed with some overhead at the bmp file size. In this project, the manipulated  information per pixel was binary. The only interesting thing was, if a pixel represents a prime number on not. This suggest a monochrome bmp, where one bit represents one pixel. This however was not featured by this library, but was neither by other libraries of similar size. The one bit information is now stored in one byte.

\subsection{Primality test}
\label{sec:prim_test}
As described in section \ref{sec:primality_test}, the primality test is highly inefficient if it is not probabilistic. The implementation of an own primality test that can handle the number representation of the \emph{gmplib} would have cost too much time. Luckily, the very same library already provided a primality test. The Miller-Rabin probabilistic primality test is applied several times to minimize the probability of false positives in the prime pattern. The theoretical possibility remains. However, the probability that several of these false positives are aligned as a pattern within the large test scope is neglectable and can be falsified by applying a non heuristic primality test, which is just a small change inside the code.

\subsection{Space filling curves}
\label{sec:curves}
The creation of the curves to which the primes were aligned to gave some challenges which will be examined in this section.
\subsubsection{Definition}
A space filling curve is defined as a path through every position in a discrete multi dimensional space with equal edge-length.%cube like

\medskip
To these space filling curves, several other criteria can be applied:
\begin{itemize}%enumerate
   \item Dimensions\\
      To how many dimension does the curve apply
   \item Continuity\\
      The path does only jump to adjacency squares
   \item Recursive\\
      The path can be generated by recursive application of a simple pattern
   \item Size restriction\\
      The curve can be scaled to any size, without losing any of its other criteria
\end{itemize}%enumerate

\subsubsection{Spiral algorithm}
The algorithm to align the prime numbers in a spiral around the center of the bitmap is fairly simple. At first, the center has to be determined and set. To actually have a center represented by a pixel, the bitmap height and width has to be odd. With every iteration of the algorithm one additional circumnavigation is done. By starting in one direction the first and second line - which lies rectangular to the first - have the length of one. The next two lines that continue the spiral in the same direction - either clockwise or counter clockwise - have one additional pixel. This pixel is the center that has not been passed by the first two lines. After these 4 line have been drawn, the length of the next line is also increased by one. In a two dimensional space the four rectangular direction have also been passed, thus the spiral is now facing in the very same direction as at the beginning. The algorithm now repeats until the calculated amount of iteration or wished pixed have been passed.

\subsubsection{Hilbert curve}
\graphic{hilbert_curve}{width=10.5cm}{Three Iteration of the Hilbert curve.}
Parallel to the criteria mentioned above, the Hilbert curve is 2 dimensional, continuous and recursive, but restricted to space sizes of $2^{n}$. The curves can be spaced to higher dimensions.

To understand the algorithm for the Hilbert curve, it is crucial to envision the derivations and changes between the single steps of the Hilbert curve. One iteration of the Hilbert curve passes a two by two pixel grid with the origin at the lower left pixel in this order: (0, 0), (1, 0), (1, 1), (0, 1). This is done by the transition of the given Axis (X, Y), starting at (0, 0) in this order: (+1, --), (-- , +1), (-1, --). These pixel scale transition are represented by a triangle facing in the direction to which the transition is done (\tt, \tl, \tb, \tr). So for traversing four pixels three transition were made.

This transition is represented by the symbol \sqr. The transition order may as well be done facing in other directions, and is then represented by the symbols \sqt, \sql and \sqb. Every one of these open squares have their own transition order. Their notation is as follows:

\begin{itemize}%enumerate
   \item \sqr $\Rightarrow$ \tr \tt \tl
   \item \sqt $\Rightarrow$ \tt \tr \tb
   \item \sql $\Rightarrow$ \tl \tb \tr
   \item \sqb $\Rightarrow$ \tb \tl \tt
\end{itemize}%enumerate

Unlike in other space filling curve, the direction of the transition is always the same. If the Hilbert curve is done with several iterations, the transition of the open square are interleaved with open squares as follows:

\begin{itemize}%enumerate
   \item \sqr $\Rightarrow$ \sqt  \tr  \sqr  \tt  \sqr  \tl \sqb
   \item \sqt $\Rightarrow$ \sqr  \tt  \sqt  \tr  \sqt  \tb  \sql
   \item \sql $\Rightarrow$ \sqb  \tl  \sql  \tb  \sql  \tr  \sqt
   \item \sqb $\Rightarrow$ \sql  \tb  \sqb  \tl  \sqb  \tt  \sqr
\end{itemize}%enumerate

Thus, the Hilbert curve is described as a grammar where the open squares are nonterminals and the triangles are terminals. However, the information of how often the nonterminal have to be exchanged has to be set globally and marks the number of iterations.

\graphic{hilbertalgorithm}{width=6cm}{Two Iteration of the Hilbert curve grammar in tree form.}

Figure \ref{fig:hilbertalgorithm} illustrates how the grammar can be interpreted as a tree with a depth equally to the iterations and a spreading factor equally to the minimum pixel size. If the tree is finished, inorder traversal reveals the sequence to rearrange the curve. 

%Thus a Hilbert curve of three iterations in generated as follows:
%\boxit{
%\sqt \hskip 10 pt  $\Rightarrow$ \\
%\sqr \tt \sqt \tr \sqt \tb \sql \hskip 10 pt $\Rightarrow$ \\
%\sqt \tr \sqr \tt \sqr \tl \sqb \hskip 10 pt \tt \hskip 10 pt \sqr \tt \sqt \tr \sqt \tb \sql \hskip 10 pt \tr \hskip 10 pt \sql \tb \sqb \tl \sqb \tt \sqr \hskip 10 pt \tb \hskip 10 pt \sqb \tl \sql \tb \sql \tr \sqt \hskip 10 pt $\Rightarrow$ \\
%\tt \tr \tb \tr \tr \tt \tl \tt \tr \tt \tl \tl \tb \tl \tt \hskip 10 pt \tt \\
%\tr \tt \tl \tt \tt \tr \tb \tr \tt \tr \tb \tb \tl \tb \tr \hskip 10 pt \tr \\
%\tl \tb \tr \tb \tb \tl \tt \tl \tb \tl \tt \tt \tr \tt \tl \hskip 10 pt \tb \\
%\tb \tl \tt \tl \tl \tb \tr \tb \tl \tb\tr \tr \tt \tr \tb 
%}{hilbert_grammar}{Derivation of three iterations of the Hilbert curve.}

Similar grammars and trees can be created for other space filling curves. If the curve can not be built by stepping from one field on the bitmap to an adjacent field, more advanced algorithms have to be used. They require a more complex computation between the steps to determine the step size. Jumping from one field to an adjacent, in respect to the current state of the pattern, is the serial approach. A parallel version would have to use a divide and conquer strategie as discussed in section \ref{sec:paralleldesign}.

\subsection{Pattern detection}
\label{sec:pattern}
The identification of visible lines within the Ulam spiral and other traversions of the prime number sequence, is set as a goal of this project. For further studies of the prime number sequence the identification of these lines can assist to evolve probabilistic primality tests which rely on the mathematical functions of the lines found in the Ulam spiral.

At first, it is of use to get an idea of what these visible lines mean in a mathematical way. The vector through an image can be described as a mathematical function or an algorithm that traverses the image. In the Ulam spiral these vectors lay at multiples of 45$^\circ$.
If one of these vectors can be identified by the eye, this means that there are significantly more points on this vector than on the others. It also may be case, that the closest parallel vector to the one seen has significantly less points and therefore the seen one gets a high contrast.

It is difficult to identify these lines with pattern detection algorithms from the field of image processing. A variety of edge detection algorithms like the Laplace filter or the Sobel operator can be found. These algorithms rely on continuous lines and blur the original image.$^{\cite{image}}$ This is not precise and reliable enough to enable further utilization. Therefore it is necessary to come up with numbers that describe the fact, that one can identify lines in the pattern. 

The implementation gives a probability to every vector. This statistic value approximates the probability, that a given point on this vector is prime number or not. The cumulation of several crossing vector then can produce an even more accurate probability, that a point represents a prime or not.

For small images and at the edges of an image, where a 45$^\circ$ vector just holds few values, this probability of course is inaccurate and has to be discarded. It was possible to clearly locate high probability vectors within the images. The corresponding figures can be found in section \ref{sec:results}.

\subsection{Parallel implementation}
\label{sec:par_imp}
The implementation of the parallel version of the software was very challenging and it turned out be be too complex to be finished in the time given. I will elaborate on these challenges and the way I addressed them. Some were implemented successfully, some had to stay in the design phase as reported in section \ref{sec:paralleldesign}. I ascribe my failure to a personal insufficiency of experience in programming the Cell processor in C and not to misconceptions in the design or defective assumptions about the software. To approve this, I will give a short word on my experiences in fore run to the project and then discuss three major challenges I faced.
I learned C because of this project and usually favor higher abstraction programming languages. Starting with Java, Lisp  and after some dissipations with Python and Prolog I currently have most experience with Haskell. Haskell is functional, pure and strictly typed. From there - without stating any superiority of any of these languages - I am not very skilled with implicit type conversions and pointer arithmetics. Both of them are heavily used when programing for the Cell processor and both are core parts of my following narrated problems. I was able to identify the problems and came up with solutions but it took too much time to implement all of them, which finally lead to an incomplete implementation.

\subsubsection{Pointers}
The explicit and implicit use of pointers is essential. Generally when writing code for the Cell processor and in special when dealing with copious data structure like bitmaps which hold pixel arrays and header information at different non subsequently heap spaces. Pointers are also used to pass and address data in the main storage which can not be accessed by the SPE. The SPE has to transfer the data to its own local store. The clear distinction between the data and pointers to this data or within this data is crucial. The arithmetics on this data also has to be figured out carefully. Especially because own data structures have to be aligned when they need to be passed between the SPE and the PPU or vice versa.This alignment in combination with multiple type definitions and the conversion between these type definition is hard to track. Without data changes of the very same bit vector on the machine the ways of accessing them vary widely. In addition, the data may be interlaced with zeros to meet the alignment criteria. I could not rely on the data size within the data structure to deduce the pointer to the data within the structure without extensive testing.

The visual representation also varied with different type definitions. It occurred that multiple data were represented as the same due to its accessing data type. This in particular is hard to identify if the data is represented as zero on the output but actually is not.

\subsubsection{Two dimensional arrays}
\label{sec:2darray}
Two dimensional array are often used in the program. To access a single pixel of the bitmap the two dimensional array is an obvious choice because the two dimensions resemble the two dimensions of the image. To conclude from this analogousness that an array is one block of data is faulty.

On the other hand, the two dimensional array serves as a great data structure to exploit the Cell processor architecture. The distinction between the first dimension, which is an array of pointers, and the second dimension, which is the actual data referenced by the first dimension, comes in handy to the polling mechanism. This mechanism is triggered by the SPE to get data from the main memory. The typically large data behind a two dimensional array can - and probably has to be - obtained in a couple of requests. The transfer of the first dimension of the array already holds the pointers needed to request portions of the data needed.

The trick here is to be aware of the different sizes the data has in the first and in the second dimension and that every data lies within the second dimension. Pointers can only navigate in one of the many arrays of the second dimension and conclusions from one second dimension array pointer to another are only possible if the difference between the references of the referencing pointers in the fist dimension are taken into account. I suggest not to conclude any pointer positions in two distinct second dimension array but to traverse every single array. 

\subsubsection{Data transfer size}
The two functions used to transfer data from and to the SPEs are \emph{mfc\_ get} and \emph{mfc\_put}. Both of them are restricted in the size. This restriction however is not the local memory limit and the data required to transfer may exceed this limitation. For both of these functions, wrappers were implemented (\emph{putback} and \emph{getfrom}) that can be requested to transfer arbitrary large data. They split the data in half and recursively call themselves on the bisect data until the data size is fallen below the limit.
% These aspects include data transfer and data alignment.
% 2d array
% pixel size

%\subsectioncd {subsection}
%\subsubsection{subsubsection}
%\paragraph{paragraph}
%\subparagraph{subparagraph}
%List
%\begin{itemize}%enumerate
%	\item Punkt1
%	\item Punkt2
%\end{itemize}%enumerate

%\graphic{label}{caption}

%Ref to Picture:
%(see:\ref{fig:XXX})

%Table
%\begin{table}[H]
%\begin{longtable} {| l l  | }
%\hline
%Topic1		& Topic2 	\\ \hline
%Cell1		& Cell2		\\
%Cell1.1	& Cell2.1	\\
%\hline
%\end{longtable}
%\end{table}
